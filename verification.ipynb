{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9309e5b0",
   "metadata": {},
   "source": [
    "# Verification of micrograd\n",
    "\n",
    "This Notebook verifies that the forward pass (prediction and loss) and backward pass (gradients) of micrograd yield the same result as pytorch.\n",
    "\n",
    "This is done by initializing the same, simple neural network in both framework (same architecture, weights, biases and loss function), and verifying that they output the same prediction, loss and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eebe8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import MLP\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb1ced",
   "metadata": {},
   "source": [
    "### Initialize the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27bf9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing micrograd neural network with random weights\n",
    "mlp = MLP(2, [2, 1])\n",
    "\n",
    "# Initializing pytorch neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_tanh_stack = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_tanh_stack(x)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = model.double() # Model should have double precision (float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7ec26",
   "metadata": {},
   "source": [
    "### Copy weights and biases from micrograd NN to pytorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e726a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_weights = []\n",
    "nn_biases = []\n",
    "for l in mlp.layers:\n",
    "    layer_weights = []\n",
    "    layer_biases = []\n",
    "    for n in l.neurons:\n",
    "        layer_weights.append([w.data for w in n.w])\n",
    "        layer_biases.append(n.b.data)\n",
    "    nn_weights.append(layer_weights)\n",
    "    nn_biases.append(layer_biases)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.linear_tanh_stack[0].weight.copy_(torch.tensor(nn_weights[0], dtype=torch.float64))\n",
    "    model.linear_tanh_stack[0].bias.copy_(torch.tensor(nn_biases[0], dtype=torch.float64))\n",
    "    model.linear_tanh_stack[2].weight.copy_(torch.tensor(nn_weights[1], dtype=torch.float64))\n",
    "    model.linear_tanh_stack[2].bias.copy_(torch.tensor(nn_biases[1], dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a05157",
   "metadata": {},
   "source": [
    "### Verify that weights and biases are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32474628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micrograd:\n",
      "\tLayer 0 weights: [[0.6744824194476045, -0.9557225745868514], [0.521792750971855, 0.24005915651984222]] - biases: [0.7535319361671708, 0.5996844557542491]\n",
      "\tLayer 1 weights: [[0.6678574714158483, -0.272827559835513]] - biases: [-0.870880270357036]\n",
      "torch:\n",
      "\tLayer 0 weights: [[0.6744824194476045, -0.9557225745868514], [0.521792750971855, 0.24005915651984222]] - biases: [0.7535319361671708, 0.5996844557542491]\n",
      "\tLayer 1 weights: [[0.6678574714158483, -0.272827559835513]] - biases: [-0.870880270357036]\n"
     ]
    }
   ],
   "source": [
    "print(\"micrograd:\")\n",
    "for i in range(len(nn_weights)):\n",
    "    print(f\"\\tLayer {i} weights: {nn_weights[i]} - biases: {nn_biases[i]}\")\n",
    "\n",
    "print(\"torch:\")\n",
    "torch.set_printoptions(precision=16)  # show up to 16 decimals\n",
    "print(f\"\\tLayer 0 weights: {model.linear_tanh_stack[0].weight.data.tolist()} - biases: {model.linear_tanh_stack[0].bias.data.tolist()}\")\n",
    "print(f\"\\tLayer 1 weights: {model.linear_tanh_stack[2].weight.data.tolist()} - biases: {model.linear_tanh_stack[2].bias.data.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed6884",
   "metadata": {},
   "source": [
    "### Define input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59f178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "x = [1.0, 0.0]\n",
    "x_tensor = torch.tensor(x, dtype=torch.float64)\n",
    "\n",
    "#  Target\n",
    "y_target = 0.0\n",
    "y_target_tensor = torch.tensor(y_target, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ffa0b",
   "metadata": {},
   "source": [
    "### Predicition (forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9692fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micrograd: \t -0.4590550574541664\n",
      "torch: \t\t -0.4590550574541663\n"
     ]
    }
   ],
   "source": [
    "pred = mlp(x)\n",
    "print(\"micrograd: \\t\", pred.data)\n",
    "\n",
    "torch_pred = model(x_tensor)\n",
    "print(\"torch: \\t\\t\", torch_pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d7b89",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27b0cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micrograd: \t 0.21073154577424802\n",
      "torch: \t\t 0.21073154577424794\n"
     ]
    }
   ],
   "source": [
    "loss = (pred - y_target)**2\n",
    "print(\"micrograd: \\t\", loss.data)\n",
    "\n",
    "torch_loss = loss_fn(torch_pred, y_target_tensor)\n",
    "print(\"torch: \\t\\t\", torch_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebf753",
   "metadata": {},
   "source": [
    "### Backward pass (calculate gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e7ecf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micrograd:\n",
      "\tLayer 0:\n",
      "\t\tWeight grad ['-0.09952857656704667', '0.0'] - bias grad: -0.09952857656704667\n",
      "\t\tWeight grad ['0.06860272014090146', '0.0'] - bias grad: 0.06860272014090146\n",
      "\tLayer 1:\n",
      "\t\tWeight grad ['-0.6458377483223697', '-0.5855648740175957'] - bias grad: -0.7246353512027274\n",
      "torch:\n",
      "\tlinear_tanh_stack.0.weight (grad): [[-0.09952857656704664, 0.0], [0.06860272014090146, 0.0]]\n",
      "\tlinear_tanh_stack.0.bias (grad): [-0.09952857656704664, 0.06860272014090146]\n",
      "\tlinear_tanh_stack.2.weight (grad): [[-0.6458377483223696, -0.5855648740175956]]\n",
      "\tlinear_tanh_stack.2.bias (grad): [-0.7246353512027273]\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "torch_loss.backward()\n",
    "\n",
    "print(\"micrograd:\")\n",
    "for i in range(len(mlp.layers)):\n",
    "    print(f\"\\tLayer {i}:\")\n",
    "    for n in mlp.layers[i].neurons:\n",
    "        param_list = [str(v.grad) for v in n.w]\n",
    "        print(f\"\\t\\tWeight grad {param_list} - bias grad: {str(n.b.grad)}\")\n",
    "\n",
    "print(\"torch:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"\\t{name} (grad): {param.grad.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
